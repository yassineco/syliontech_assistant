import React, { useState, useCallback, useRef, useEffect, useMemo } from 'react';
import { Mic, MicOff, Send, VolumeX } from 'lucide-react';
import { sendMessage } from '../lib/api';

interface Message {
  id: string;
  type: 'user' | 'assistant';
  content: string;
  timestamp: Date;
}

interface SimpleVoiceAssistantProps {
  onMessage?: (message: string) => Promise<string>;
}

export function SimpleVoiceAssistant({ onMessage }: SimpleVoiceAssistantProps) {
  const [messages, setMessages] = useState<Message[]>([]);
  const [inputText, setInputText] = useState('');
  const [isProcessing, setIsProcessing] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [microphoneStatus, setMicrophoneStatus] = useState<'unknown' | 'granted' | 'denied' | 'prompt'>('unknown');
  const [errorMessage, setErrorMessage] = useState<string>('');
  const [isAutoMode, setIsAutoMode] = useState(false);
  const [currentTranscript, setCurrentTranscript] = useState('');
  
  // Refs pour la reconnaissance vocale
  const recognitionRef = useRef<any>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const silenceTimerRef = useRef<number | null>(null);
  const autoRestartTimerRef = useRef<number | null>(null);
  
  // Support navigateur avec d√©tection avanc√©e
  const isSupported = Boolean(
    typeof window !== 'undefined' &&
    (window.SpeechRecognition || window.webkitSpeechRecognition) &&
    window.speechSynthesis
  );

  // D√©tection des capacit√©s du navigateur
  const browserCapabilities = useMemo(() => {
    if (typeof window === 'undefined') return { name: 'unknown', features: [] };
    
    const userAgent = navigator.userAgent;
    const features = [];
    
    if (window.SpeechRecognition) features.push('native-speech');
    if (window.webkitSpeechRecognition) features.push('webkit-speech');
    if (window.speechSynthesis) features.push('speech-synthesis');
    if ((window as any).AudioContext || (window as any).webkitAudioContext) features.push('audio-context');
    
    const browser = userAgent.includes('Chrome') ? 'chrome' :
                   userAgent.includes('Firefox') ? 'firefox' :
                   userAgent.includes('Safari') ? 'safari' :
                   userAgent.includes('Edge') ? 'edge' : 'unknown';
    
    return { name: browser, features };
  }, []);

  console.log('üåê Capacit√©s navigateur:', browserCapabilities);

  // Feedback audio pour les actions utilisateur (D√âSACTIV√â temporairement)
  const playFeedbackSound = useCallback((type: 'start' | 'stop' | 'send' | 'error') => {
    // D√©sactiv√© pour √©viter les bips intempestifs
    return;
    
    try {
      const audioContext = new ((window as any).AudioContext || (window as any).webkitAudioContext)();
      const oscillator = audioContext.createOscillator();
      const gainNode = audioContext.createGain();
      
      oscillator.connect(gainNode);
      gainNode.connect(audioContext.destination);
      
      // Configuration selon le type de feedback
      switch (type) {
        case 'start':
          oscillator.frequency.setValueAtTime(800, audioContext.currentTime);
          oscillator.frequency.setValueAtTime(1000, audioContext.currentTime + 0.1);
          gainNode.gain.setValueAtTime(0.1, audioContext.currentTime);
          break;
        case 'stop':
          oscillator.frequency.setValueAtTime(1000, audioContext.currentTime);
          oscillator.frequency.setValueAtTime(800, audioContext.currentTime + 0.1);
          gainNode.gain.setValueAtTime(0.1, audioContext.currentTime);
          break;
        case 'send':
          oscillator.frequency.setValueAtTime(600, audioContext.currentTime);
          oscillator.frequency.setValueAtTime(900, audioContext.currentTime + 0.1);
          oscillator.frequency.setValueAtTime(1200, audioContext.currentTime + 0.2);
          gainNode.gain.setValueAtTime(0.08, audioContext.currentTime);
          break;
        case 'error':
          oscillator.frequency.setValueAtTime(300, audioContext.currentTime);
          oscillator.frequency.setValueAtTime(250, audioContext.currentTime + 0.2);
          gainNode.gain.setValueAtTime(0.1, audioContext.currentTime);
          break;
      }
      
      oscillator.start(audioContext.currentTime);
      oscillator.stop(audioContext.currentTime + 0.3);
      
      console.log(`üîä Feedback audio: ${type}`);
    } catch (error) {
      // Feedback audio optionnel - pas critique
      console.log('‚ö†Ô∏è Feedback audio non disponible');
    }
  }, []);

  // V√©rification des permissions microphone
  const checkMicrophonePermissions = useCallback(async () => {
    try {
      console.log('üîç V√©rification permissions microphone...');
      
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        setErrorMessage('‚ùå Media Devices API non support√© par ce navigateur');
        setMicrophoneStatus('denied');
        return false;
      }

      // V√©rifier les permissions
      if (navigator.permissions) {
        const permission = await navigator.permissions.query({ name: 'microphone' as PermissionName });
        console.log('üé§ Permission microphone:', permission.state);
        setMicrophoneStatus(permission.state as any);

        if (permission.state === 'denied') {
          setErrorMessage('‚ùå Acc√®s microphone refus√©. Autorisez dans les param√®tres du navigateur.');
          return false;
        }
      }

      // Test d'acc√®s au microphone
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        console.log('‚úÖ Acc√®s microphone accord√©');
        setMicrophoneStatus('granted');
        setErrorMessage('');
        
        // Arr√™ter le stream imm√©diatement
        stream.getTracks().forEach(track => track.stop());
        return true;
      } catch (error: any) {
        console.error('‚ùå Erreur acc√®s microphone:', error);
        setMicrophoneStatus('denied');
        
        if (error.name === 'NotAllowedError') {
          setErrorMessage('‚ùå Acc√®s microphone refus√©. Cliquez sur l\'ic√¥ne üîí dans la barre d\'adresse.');
        } else if (error.name === 'NotFoundError') {
          setErrorMessage('‚ùå Aucun microphone d√©tect√© sur cet appareil.');
        } else {
          setErrorMessage(`‚ùå Erreur microphone: ${error.message}`);
        }
        return false;
      }
    } catch (error) {
      console.error('‚ùå Erreur v√©rification permissions:', error);
      setErrorMessage('‚ùå Impossible de v√©rifier les permissions microphone');
      setMicrophoneStatus('denied');
      return false;
    }
  }, []);

  // Initialisation de la reconnaissance vocale
  const initRecognition = useCallback(() => {
    if (!isSupported) {
      console.error('‚ùå Speech Recognition non support√©');
      setErrorMessage('‚ùå Reconnaissance vocale non support√©e par ce navigateur');
      return null;
    }

    console.log('üîß Initialisation Speech Recognition...');
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const recognition = new SpeechRecognition() as any;
    
    // Configuration optimis√©e pour sensibilit√© maximale
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'fr-FR';
    recognition.maxAlternatives = 10; // Plus d'alternatives pour meilleure pr√©cision
    
    // Param√®tres avanc√©s si disponibles (webkit)
    if (recognition.serviceURI) {
      recognition.serviceURI = 'wss://www.google.com/speech-api/v2/recognize';
    }
    
    // Configuration audio avanc√©e si support√©e
    try {
      // Forcer l'utilisation de contraintes audio optimales
      recognition.audioTrack = true;
      recognition.noiseSuppressionConstraint = true;
      recognition.echoCancellationConstraint = true;
    } catch (error) {
      console.log('‚ö†Ô∏è Param√®tres audio avanc√©s non support√©s');
    }
    
    console.log('‚öôÔ∏è Configuration Speech Recognition optimis√©e:', {
      continuous: recognition.continuous,
      interimResults: recognition.interimResults,
      lang: recognition.lang,
      maxAlternatives: recognition.maxAlternatives,
      audioOptimized: true
    });

    recognition.onstart = () => {
      console.log('üé§ ‚úÖ Enregistrement d√©marr√© avec succ√®s');
      setIsRecording(true);
      setErrorMessage('');
    };

    recognition.onend = () => {
      console.log('üé§ ‚èπÔ∏è Enregistrement termin√©');
      setIsRecording(false);
    };

    recognition.onaudiostart = () => {
      console.log('üéµ Audio captur√©');
    };

    recognition.onaudioend = () => {
      console.log('üéµ Fin capture audio');
    };

    recognition.onsoundstart = () => {
      console.log('üîä Son d√©tect√©');
    };

    recognition.onsoundend = () => {
      console.log('üîá Fin du son');
    };

    recognition.onspeechstart = () => {
      console.log('üó£Ô∏è Parole d√©tect√©e !');
    };

    recognition.onspeechend = () => {
      console.log('üó£Ô∏è Fin de parole');
    };

      recognition.onresult = (event: any) => {
        console.log('üìù R√©sultat re√ßu:', event);
        
        let finalTranscript = '';
        let interimTranscript = '';
        let maxConfidence = 0;
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          const transcript = result[0].transcript;
          const confidence = result[0].confidence || 0;
          maxConfidence = Math.max(maxConfidence, confidence);
          
          if (result.isFinal) {
            finalTranscript += transcript;
            console.log('‚úÖ Texte final:', transcript, `(confiance: ${confidence.toFixed(2)})`);
          } else {
            interimTranscript += transcript;
            console.log('‚è≥ Texte interim:', transcript, `(confiance: ${confidence.toFixed(2)})`);
          }
        }
        
        // Mise √† jour du transcript affich√©
        const fullTranscript = finalTranscript || interimTranscript;
        setCurrentTranscript(fullTranscript);
        setInputText(fullTranscript);
        
        // Logique d'auto-envoi adaptative
        const wordCount = fullTranscript.trim().split(/\s+/).length;
        const isSignificant = wordCount >= 2; // Seuil r√©duit de 3 √† 2 mots
        const hasGoodConfidence = maxConfidence > 0.5 || finalTranscript; // Confiance > 50% ou texte final
        
        // Si on a un texte final significatif avec bonne confiance
        if (finalTranscript && isSignificant && hasGoodConfidence) {
          console.log('üöÄ Auto-envoi activ√©:', {
            text: finalTranscript,
            wordCount,
            confidence: maxConfidence.toFixed(2)
          });
          
          // Arr√™ter le timer de silence s'il existe
          if (silenceTimerRef.current) {
            clearTimeout(silenceTimerRef.current);
            silenceTimerRef.current = null;
          }
          
          // Envoyer le message automatiquement
          setTimeout(() => {
            triggerAutoSend(finalTranscript.trim());
            setCurrentTranscript('');
            
            // Red√©marrer l'√©coute apr√®s envoi si en mode auto
            if (isAutoMode && !isSpeaking) {
              autoRestartTimerRef.current = setTimeout(() => {
                startListening(true); // Silent restart
              }, 1500);
            }
          }, 100);
        }
        // Si on a un texte interim significatif, d√©marrer le timer de silence adaptatif
        else if (interimTranscript && wordCount >= 2) {
          console.log('‚è±Ô∏è Timer silence adaptatif...', {
            text: interimTranscript,
            wordCount,
            confidence: maxConfidence.toFixed(2)
          });
          
          if (silenceTimerRef.current) {
            clearTimeout(silenceTimerRef.current);
          }
          
          // Timer adaptatif selon la longueur du texte
          const silenceDuration = wordCount <= 3 ? 2500 : 2000; // 2.5s pour phrases courtes, 2s pour longues
          
          silenceTimerRef.current = setTimeout(() => {
            if (currentTranscript.trim() && wordCount >= 2) {
              console.log('‚è∞ Timeout silence adaptatif - envoi auto:', {
                text: currentTranscript,
                wordCount,
                duration: silenceDuration
              });
              triggerAutoSend(currentTranscript.trim());
              setCurrentTranscript('');
              
              if (isAutoMode && !isSpeaking) {
                autoRestartTimerRef.current = setTimeout(() => {
                  startListening(true); // Silent auto-restart
                }, 1500);
              }
            }
          }, silenceDuration);
        }
      };    recognition.onerror = (event: any) => {
      console.error('‚ùå Erreur Speech Recognition:', event);
      console.error('üîç D√©tails erreur:', {
        error: event.error,
        message: event.message,
        timeStamp: event.timeStamp,
        type: event.type
      });
      
      setIsRecording(false);
      
      const errorMap: Record<string, string> = {
        'not-allowed': '‚ùå Acc√®s microphone refus√©. Cliquez sur l\'ic√¥ne üîí dans la barre d\'adresse pour autoriser.',
        'no-speech': '‚ö†Ô∏è Aucune parole d√©tect√©e. Parlez plus fort ou v√©rifiez votre microphone.',
        'audio-capture': '‚ùå Impossible de capturer l\'audio. V√©rifiez que votre microphone fonctionne.',
        'network': '‚ùå Erreur r√©seau. V√©rifiez votre connexion Internet.',
        'service-not-allowed': '‚ùå Service de reconnaissance vocale non autoris√©.',
        'bad-grammar': '‚ùå Erreur de grammaire dans la reconnaissance.',
        'language-not-supported': '‚ùå Langue fran√ßaise non support√©e.',
        'aborted': '‚ö†Ô∏è Reconnaissance interrompue.',
        'not-supported': '‚ùå Reconnaissance vocale non support√©e.'
      };
      
      const userMessage = errorMap[event.error] || `‚ùå Erreur inconnue: ${event.error}`;
      setErrorMessage(userMessage);
      
      // Tentative de red√©marrage automatique pour certaines erreurs
      if (event.error === 'aborted' || event.error === 'no-speech') {
        setTimeout(() => {
          console.log('üîÑ Tentative de red√©marrage automatique...');
          if (!isRecording && recognitionRef.current) { 
            try {
              recognitionRef.current.start();
            } catch (restartError) {
              console.error('‚ùå √âchec red√©marrage auto:', restartError);
            }
          }
        }, 2000);
      }
    };

    console.log('‚úÖ Speech Recognition initialis√©');
    return recognition;
  }, [isSupported]);

  // Fonction pour d√©marrer l'√©coute automatique
  const startListening = useCallback(async (silent = false) => {
    if (!isSupported) {
      setErrorMessage('‚ùå Reconnaissance vocale non support√©e');
      return;
    }

    // V√©rifier les permissions
    const hasPermission = await checkMicrophonePermissions();
    if (!hasPermission) {
      return;
    }

    // Si d√©j√† en cours, arr√™ter d'abord
    if (isRecording) {
      stopListening();
      return;
    }

    try {
      console.log('üéØ D√©marrage √©coute automatique...');
      setErrorMessage('üé§ √âcoute en cours - Parlez naturellement...');
      
      // Feedback audio uniquement si action manuelle (pas silent)
      if (!silent) {
        playFeedbackSound('start');
      }
      
      const recognition = initRecognition();
      if (!recognition) return;

      recognitionRef.current = recognition;
      recognition.start();
      
    } catch (error) {
      console.error('‚ùå Erreur d√©marrage √©coute:', error);
      setErrorMessage('‚ùå Impossible de d√©marrer l\'√©coute');
    }
  }, [isSupported, isRecording, checkMicrophonePermissions, initRecognition, playFeedbackSound]);

  // Fonction pour arr√™ter l'√©coute
  const stopListening = useCallback(() => {
    console.log('üõë Arr√™t √©coute...');
    
    // Feedback audio d'arr√™t
    playFeedbackSound('stop');
    
    // Arr√™ter tous les timers
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    
    if (autoRestartTimerRef.current) {
      clearTimeout(autoRestartTimerRef.current);
      autoRestartTimerRef.current = null;
    }
    
    // Arr√™ter la reconnaissance
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
        recognitionRef.current = null;
      } catch (error) {
        console.error('Erreur arr√™t recognition:', error);
      }
    }
    
    setIsRecording(false);
    setCurrentTranscript('');
    setErrorMessage('‚úÖ Pr√™t ! Activez le mode auto pour une √©coute continue');
  }, [playFeedbackSound]);

  // Toggle mode automatique
  const toggleAutoMode = useCallback(async () => {
    const newAutoMode = !isAutoMode;
    setIsAutoMode(newAutoMode);
    
    if (newAutoMode) {
      console.log('üîÑ Activation mode automatique');
      setErrorMessage('üîÑ Mode automatique activ√© - D√©marrage...');
      await startListening(false); // Avec bip car action manuelle
    } else {
      console.log('‚è∏Ô∏è D√©sactivation mode automatique');
      stopListening();
      setErrorMessage('‚è∏Ô∏è Mode automatique d√©sactiv√©');
    }
  }, [isAutoMode, startListening, stopListening]);

  // D√©marrer/arr√™ter l'enregistrement vocal - Version simplifi√©e
  const toggleRecording = useCallback(async () => {
    console.log('üé§ Toggle Recording appel√© - √âtat actuel:', { isRecording, isSupported, microphoneStatus });
    
    if (!isSupported) {
      alert('‚ùå Reconnaissance vocale non support√©e par ce navigateur');
      return;
    }

    if (isRecording) {
      // Arr√™ter l'enregistrement
      console.log('üõë Arr√™t enregistrement');
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop();
          recognitionRef.current = null;
          setIsRecording(false);
        } catch (error) {
          console.error('Erreur arr√™t:', error);
        }
      }
      return;
    }

    // D√©marrer l'enregistrement
    try {
      console.log('ÔøΩ D√©marrage enregistrement...');
      setErrorMessage('üé§ Pr√©paration...');
      
      // Cr√©er une nouvelle instance
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognition() as any;
      
      // Configuration am√©lior√©e pour meilleure d√©tection
      recognition.lang = 'fr-FR';
      recognition.continuous = true; // Chang√© √† true pour √©couter plus longtemps
      recognition.interimResults = true; // Chang√© √† true pour voir les r√©sultats partiels
      recognition.maxAlternatives = 3;
      
      console.log('‚öôÔ∏è Config reconnaissance:', {
        lang: recognition.lang,
        continuous: recognition.continuous,
        interimResults: recognition.interimResults
      });
      
      // Event handlers am√©lior√©s
      recognition.onstart = () => {
        console.log('‚úÖ Reconnaissance d√©marr√©e');
        setIsRecording(true);
        setErrorMessage('üî¥ Je vous √©coute - Parlez FORT et CLAIREMENT !');
      };
      
      recognition.onaudiostart = () => {
        console.log('üéµ Audio capture d√©marr√©e');
      };
      
      recognition.onsoundstart = () => {
        console.log('üîä Son d√©tect√© !');
        setErrorMessage('üîä Son d√©tect√© - continuez √† parler...');
      };
      
      recognition.onspeechstart = () => {
        console.log('üó£Ô∏è PAROLE D√âTECT√âE !');
        setErrorMessage('üó£Ô∏è Parole d√©tect√©e - parfait !');
      };
      
      recognition.onspeechend = () => {
        console.log('üó£Ô∏è Fin de parole d√©tect√©e');
        setErrorMessage('üó£Ô∏è Fin de parole - traitement...');
      };
      
      recognition.onsoundend = () => {
        console.log('üîá Fin du son');
      };
      
      recognition.onaudioend = () => {
        console.log('üéµ Fin capture audio');
      };
      
      recognition.onend = () => {
        console.log('‚èπÔ∏è Reconnaissance termin√©e');
        setIsRecording(false);
        setErrorMessage('‚úÖ Pr√™t ! Cliquez pour parler √† nouveau');
      };
      
      recognition.onresult = (event: any) => {
        console.log('üìä Event result re√ßu:', event);
        console.log('üìä Nombre de r√©sultats:', event.results.length);
        
        for (let i = 0; i < event.results.length; i++) {
          const result = event.results[i];
          const text = result[0].transcript.trim();
          const confidence = result[0].confidence;
          
          console.log(`üìù R√©sultat ${i}:`, {
            text,
            confidence,
            isFinal: result.isFinal
          });
          
          // Mise √† jour en temps r√©el
          setInputText(text);
          
          if (result.isFinal) {
            console.log('‚úÖ R√©sultat final:', text);
            setErrorMessage(`‚úÖ Reconnu: "${text}"`);
            
            // Auto-envoi si assez de mots
            if (text && text.split(' ').length >= 3) {
              console.log('üöÄ Auto-envoi d√©clench√© pour:', text);
              setTimeout(() => triggerAutoSend(text), 100);
              // Arr√™ter la reconnaissance apr√®s envoi
              recognition.stop();
            }
          } else {
            setErrorMessage(`üé§ En cours: "${text}..."`);
          }
        }
      };
      
      recognition.onerror = (event: any) => {
        console.error('‚ùå Erreur recognition:', event.error);
        setIsRecording(false);
        setErrorMessage(`‚ùå Erreur: ${event.error}`);
      };
      
      // Stocker et d√©marrer
      recognitionRef.current = recognition;
      recognition.start();
      
    } catch (error) {
      console.error('‚ùå Erreur cr√©ation recognition:', error);
      setErrorMessage(`‚ùå Erreur: ${error}`);
      setIsRecording(false);
    }
  }, [isRecording, isSupported]);

  // Auto-d√©marrage de l'√©coute vocale au montage du composant
  useEffect(() => {
    // Version simplifi√©e - pas d'auto-d√©marrage pour √©viter les conflits
    console.log('üé§ Composant mont√© - pr√™t pour interaction manuelle');
    
    // Simple v√©rification des permissions au montage
    if (isSupported) {
      checkMicrophonePermissions().then(hasPermission => {
        if (hasPermission) {
          setErrorMessage('‚úÖ Pr√™t ! Cliquez sur le microphone pour parler');
        } else {
          setErrorMessage('üé§ Cliquez sur le microphone pour autoriser l\'acc√®s');
        }
      }).catch(() => {
        setErrorMessage('‚ö†Ô∏è Probl√®me permissions - utilisez le bouton microphone');
      });
    }
  }, []); // Pas de d√©pendances pour √©viter les re-renders

  // Synth√®se vocale optimis√©e avec gestion des interruptions
  const speak = useCallback((text: string) => {
    if (!text.trim() || isSpeaking) return;

    // Nettoyer le texte pour une prononciation optimale
    const cleanText = text
      .replace(/\*\*/g, '') // Supprimer le markdown
      .replace(/\*/g, '')
      .replace(/#{1,6}\s/g, '')
      .replace(/\bcr√©dits?\b/gi, 'cr√©di') // cr√©dit -> cr√©di (√©vite √©pellation)
      .replace(/TAEG/gi, 'T.A.E.G.') // √âpeler l'acronyme
      .replace(/APR/gi, 'A.P.R.')
      .replace(/\b(\d+)\s*‚Ç¨/gi, '$1 euros')
      .replace(/\b(\d+)\s*%/gi, '$1 pour cent')
      .replace(/24h/gi, '24 heures')
      .replace(/\n+/g, '. ') // Remplacer les retours ligne par des points
      .replace(/\s+/g, ' ') // Normaliser les espaces
      .trim();

    console.log('üîä D√©marrage synth√®se vocale optimis√©e:', cleanText.substring(0, 50) + '...');
    setIsSpeaking(true);

    // S√©lectionner la meilleure voix fran√ßaise
    const voices = speechSynthesis.getVoices();
    const frenchVoice = voices.find(voice => 
      voice.lang.startsWith('fr') && (voice.name.includes('Google') || voice.name.includes('Am√©lie'))
    ) || voices.find(voice => voice.lang.startsWith('fr'));

    const utterance = new SpeechSynthesisUtterance(cleanText);
    utterance.voice = frenchVoice || null;
    utterance.lang = 'fr-FR';
    utterance.rate = 0.85; // L√©g√®rement plus lent pour clart√©
    utterance.pitch = 1.0;
    utterance.volume = 0.8;

    // Gestion interruption par nouvelle parole utilisateur
    utterance.onstart = () => {
      console.log('üîä Synth√®se d√©marr√©e - √©coute des interruptions...');
      
      // √âcouter pour interruptions si en mode auto
      if (isAutoMode && !isRecording) {
        setTimeout(() => {
          // D√©marrer une √©coute discr√®te pendant la synth√®se
          const interruptionRecognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
          interruptionRecognition.continuous = false;
          interruptionRecognition.interimResults = true;
          interruptionRecognition.lang = 'fr-FR';
          
          interruptionRecognition.onresult = (event: any) => {
            if (event.results.length > 0) {
              const transcript = event.results[0][0].transcript.trim();
              if (transcript.length > 3) {
                console.log('üõë Interruption d√©tect√©e:', transcript);
                speechSynthesis.cancel();
                setIsSpeaking(false);
                
                // Traiter la nouvelle demande
                if (transcript.split(/\s+/).length >= 2) {
                  setTimeout(() => {
                    triggerAutoSend(transcript);
                  }, 300);
                }
              }
            }
          };
          
          interruptionRecognition.onerror = () => {
            // Erreur silencieuse - pas critique
          };
          
          try {
            interruptionRecognition.start();
          } catch (error) {
            // Pas critique
          }
        }, 500);
      }
    };

    utterance.onend = () => {
      console.log('üîä Synth√®se termin√©e');
      setIsSpeaking(false);
      
      // Red√©marrer l'√©coute automatiquement si en mode auto
      if (isAutoMode && !isRecording) {
        setTimeout(() => {
          startListening(true); // Silent auto-restart apr√®s synth√®se
        }, 800);
      }
    };

    utterance.onerror = (error) => {
      console.error('‚ùå Erreur synth√®se:', error);
      setIsSpeaking(false);
      
      // Red√©marrer l'√©coute m√™me en cas d'erreur
      if (isAutoMode && !isRecording) {
        setTimeout(() => {
          startListening(true); // Silent auto-restart apr√®s erreur
        }, 1000);
      }
    };

    speechSynthesis.cancel(); // Arr√™ter toute synth√®se en cours
    speechSynthesis.speak(utterance);
  }, [isSpeaking, isAutoMode, isRecording, startListening]);

  // Arr√™ter la synth√®se vocale avec red√©marrage intelligent
  const stopSpeaking = useCallback(() => {
    console.log('üõë Arr√™t manuel synth√®se');
    speechSynthesis.cancel();
    setIsSpeaking(false);
    
    // Red√©marrer l'√©coute si en mode auto
    if (isAutoMode && !isRecording) {
      setTimeout(() => {
        startListening(true); // Silent auto-restart apr√®s arr√™t manuel
      }, 500);
    }
  }, [isAutoMode, isRecording, startListening]);

  // Ajouter un message
  const addMessage = useCallback((type: 'user' | 'assistant', content: string) => {
    const newMessage: Message = {
      id: Date.now().toString(),
      type,
      content,
      timestamp: new Date()
    };
    setMessages(prev => [...prev, newMessage]);
    return newMessage;
  }, []);

  // Auto-envoi apr√®s reconnaissance vocale
  const triggerAutoSend = useCallback(async (text: string) => {
    if (!text || isProcessing) return;

    console.log('üì§ Auto-envoi:', text);
    
    // Feedback audio d'envoi
    playFeedbackSound('send');
    
    setIsProcessing(true);

    // Ajouter le message utilisateur
    addMessage('user', text);

    try {
      let response: string;
      
      if (onMessage) {
        // Utiliser la fonction personnalis√©e si fournie
        response = await onMessage(text);
      } else {
        // Utiliser l'API Sofinco par d√©faut
        const conversationHistory = messages.map(msg => ({
          role: msg.type as 'user' | 'assistant',
          message: msg.content,
          timestamp: msg.timestamp.toISOString()
        }));
        
        const apiResponse = await sendMessage(text, undefined, undefined, conversationHistory);
        response = apiResponse.reply || 'D√©sol√©, je n\'ai pas pu traiter votre demande.';
      }
      
      if (response) {
        // Ajouter la r√©ponse
        addMessage('assistant', response);
        
        // Lire automatiquement la r√©ponse
        setTimeout(() => {
          speak(response);
        }, 500); // Petit d√©lai pour √©viter les conflits
      }
    } catch (error) {
      console.error('Erreur API:', error);
      const errorMsg = 'D√©sol√©, je rencontre un probl√®me technique.';
      addMessage('assistant', errorMsg);
      speak(errorMsg);
    } finally {
      setIsProcessing(false);
      // Vider le champ de texte apr√®s l'envoi automatique
      setInputText('');
    }
  }, [isProcessing, addMessage, onMessage, speak, messages]);

  // Envoyer un message
  const handleSend = useCallback(async () => {
    const text = inputText.trim();
    if (!text || isProcessing) return;

    console.log('üì§ Envoi:', text);
    setIsProcessing(true);
    setInputText('');

    // Ajouter le message utilisateur
    addMessage('user', text);

    try {
      let response: string;
      
      if (onMessage) {
        // Utiliser la fonction personnalis√©e si fournie
        response = await onMessage(text);
      } else {
        // Utiliser l'API Sofinco par d√©faut
        const conversationHistory = messages.map(msg => ({
          role: msg.type as 'user' | 'assistant',
          message: msg.content,
          timestamp: msg.timestamp.toISOString()
        }));
        
        const apiResponse = await sendMessage(text, undefined, undefined, conversationHistory);
        response = apiResponse.reply || 'D√©sol√©, je n\'ai pas pu traiter votre demande.';
      }
      
      if (response) {
        // Ajouter la r√©ponse
        addMessage('assistant', response);
        
        // Lire automatiquement la r√©ponse
        setTimeout(() => {
          speak(response);
        }, 500); // Petit d√©lai pour √©viter les conflits
      }
    } catch (error) {
      console.error('Erreur API:', error);
      const errorMsg = 'D√©sol√©, je rencontre un probl√®me technique.';
      addMessage('assistant', errorMsg);
      speak(errorMsg);
    } finally {
      setIsProcessing(false);
    }
  }, [inputText, isProcessing, addMessage, onMessage, speak, messages]);

  // Gestion du formulaire
  const handleSubmit = useCallback((e: React.FormEvent) => {
    e.preventDefault();
    handleSend();
  }, [handleSend]);

  // Effet d'initialisation automatique
  useEffect(() => {
    const initializeAutoMode = async () => {
      // Attendre un peu pour que le composant soit mont√©
      await new Promise(resolve => setTimeout(resolve, 1000));
      
      if (isSupported && microphoneStatus === 'unknown') {
        console.log('üöÄ Initialisation automatique...');
        const hasPermission = await checkMicrophonePermissions();
        
        if (hasPermission) {
          console.log('üîÑ Activation mode automatique initial');
          setIsAutoMode(true);
          setErrorMessage('üîÑ Mode automatique activ√© - Parlez naturellement !');
          
          // D√©marrer l'√©coute apr√®s un petit d√©lai (silencieux au d√©marrage)
          setTimeout(() => {
            startListening(true); // Silent initial start
          }, 1000);
        }
      }
    };
    
    initializeAutoMode();
  }, [isSupported, microphoneStatus, checkMicrophonePermissions, startListening]);

  // Nettoyage √† la destruction du composant
  useEffect(() => {
    return () => {
      // Nettoyer tous les timers et ressources
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
      }
      if (autoRestartTimerRef.current) {
        clearTimeout(autoRestartTimerRef.current);
      }
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop();
        } catch (error) {
          console.error('Erreur nettoyage:', error);
        }
      }
    };
  }, []);

  return (
    <div className="flex flex-col h-full max-w-4xl mx-auto bg-white rounded-lg shadow-lg">
      {/* En-t√™te simplifi√© */}
      <div className="bg-green-600 text-white p-4 rounded-t-lg">
        <h1 className="text-xl font-bold">üé§ Assistant Vocal SylionTech</h1>
        
        {/* Message principal */}
        <p className="text-green-100 text-sm mt-2">
          {errorMessage || (isRecording ? 'üî¥ Je vous √©coute...' : 'üé§ Cliquez sur le microphone pour parler')}
        </p>
        
        {/* Statut simple */}
        <div className="text-green-200 text-xs mt-2 flex items-center gap-4">
          <span>‚ú® Version simplifi√©e</span>
          {isProcessing && <span className="animate-pulse">‚è≥ Traitement...</span>}
        </div>
      </div>

      {/* Zone de messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4 min-h-[400px]">
        {messages.length === 0 && (
          <div className="text-center text-gray-500 mt-8">
            <div className="text-4xl mb-4">üé§</div>
            <p className="text-lg font-medium">Bonjour ! Je suis votre assistant SylionTech.</p>
            <p className="text-sm mt-2">Cliquez sur le microphone et parlez-moi de votre projet de financement.</p>
          </div>
        )}
        
        {messages.map((message) => (
          <div
            key={message.id}
            className={`flex ${message.type === 'user' ? 'justify-end' : 'justify-start'}`}
          >
            <div
              className={`max-w-xs lg:max-w-md px-4 py-2 rounded-lg ${
                message.type === 'user'
                  ? 'bg-blue-600 text-white'
                  : 'bg-gray-100 text-gray-800'
              }`}
            >
              <p className="whitespace-pre-wrap">{message.content}</p>
              <p className="text-xs opacity-70 mt-1">
                {message.timestamp.toLocaleTimeString()}
              </p>
            </div>
          </div>
        ))}
        
        {isProcessing && (
          <div className="flex justify-start">
            <div className="bg-gray-100 text-gray-800 px-4 py-2 rounded-lg">
              <div className="flex items-center space-x-2">
                <div className="animate-spin rounded-full h-4 w-4 border-b-2 border-blue-600"></div>
                <span>Je r√©fl√©chis...</span>
              </div>
            </div>
          </div>
        )}
      </div>

      {/* Zone de saisie */}
      <div className="border-t bg-gray-50 p-4">
        <form onSubmit={handleSubmit}>
        <div className="flex items-center space-x-2">
          {/* Bouton mode automatique */}
          <button
            type="button"
            onClick={toggleAutoMode}
            disabled={isProcessing || !isSupported}
            className={`px-4 py-2 rounded-lg font-medium transition-all duration-200 ${
              isAutoMode
                ? 'bg-blue-500 hover:bg-blue-600 text-white'
                : 'bg-gray-200 hover:bg-gray-300 text-gray-700'
            } ${!isSupported ? 'opacity-50 cursor-not-allowed' : ''}`}
            title={isAutoMode ? 'D√©sactiver mode automatique' : 'Activer mode automatique'}
          >
            {isAutoMode ? 'üîÑ AUTO' : '‚è∏Ô∏è MANUEL'}
          </button>

          {/* Bouton microphone */}
          <button
            type="button"
            onClick={isAutoMode ? stopListening : toggleRecording}
            disabled={isProcessing || !isSupported}
            className={`p-3 rounded-full transition-all duration-200 ${
              isRecording
                ? 'bg-red-500 hover:bg-red-600 text-white animate-pulse'
                : 'bg-green-500 hover:bg-green-600 text-white'
            } ${!isSupported ? 'opacity-50 cursor-not-allowed' : ''}`}
            title={
              isAutoMode 
                ? (isRecording ? '√âcoute automatique en cours' : 'Red√©marrer √©coute auto')
                : (isRecording ? 'Arr√™ter l\'enregistrement' : 'Commencer √† parler')
            }
          >
            {isRecording ? <MicOff className="w-5 h-5" /> : <Mic className="w-5 h-5" />}
          </button>

          {/* Bouton diagnostic */}
          <button
            type="button"
            onClick={async () => {
              console.log('üîß DIAGNOSTIC FORC√â...');
              console.log('üîß Support navigateur:', isSupported);
              console.log('üîß √âtat microphone:', microphoneStatus);
              console.log('üîß Reconnaissance actuelle:', recognitionRef.current);
              
              // Test permissions
              const hasPermission = await checkMicrophonePermissions();
              console.log('üîß Test permissions r√©sultat:', hasPermission);
              
              // Test Speech Recognition
              if (isSupported) {
                try {
                  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                  const testRecognition = new SpeechRecognition() as any;
                  console.log('üîß Nouvelle instance SpeechRecognition cr√©√©e:', testRecognition);
                  
                  testRecognition.lang = 'fr-FR';
                  testRecognition.continuous = false;
                  testRecognition.interimResults = false;
                  
                  testRecognition.onstart = () => console.log('üîß TEST: Recognition started');
                  testRecognition.onend = () => console.log('üîß TEST: Recognition ended');
                  testRecognition.onresult = (e: any) => console.log('üîß TEST: Result:', e);
                  testRecognition.onerror = (e: any) => console.log('üîß TEST: Error:', e);
                  
                  testRecognition.start();
                  console.log('üîß Test recognition d√©marr√©');
                } catch (error) {
                  console.error('üîß TEST ERROR:', error);
                }
              }
            }}
            className="px-3 py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600 text-sm"
            title="Diagnostic complet"
          >
            üîß
          </button>

          {/* Champ de saisie */}
          <input
            type="text"
            value={inputText}
            onChange={(e) => setInputText(e.target.value)}
            placeholder="Discussion vocale active - Parlez ou tapez..."
            className="flex-1 border border-gray-300 rounded-lg px-4 py-2 focus:outline-none focus:border-green-500"
            disabled={isProcessing}
          />

          {/* Bouton envoi */}
          <button
            type="submit"
            disabled={!inputText.trim() || isProcessing}
            className="px-4 py-2 bg-green-600 text-white rounded-lg hover:bg-green-700 disabled:opacity-50 disabled:cursor-not-allowed transition-colors"
            title="Envoi manuel (ou auto apr√®s vocal)"
          >
            <Send className="w-5 h-5" />
          </button>

          {/* Bouton arr√™ter synth√®se */}
          {isSpeaking && (
            <button
              type="button"
              onClick={stopSpeaking}
              className="px-4 py-2 bg-red-500 text-white rounded-lg hover:bg-red-600 transition-colors"
              title="Arr√™ter la lecture"
            >
              <VolumeX className="w-5 h-5" />
            </button>
          )}
        </div>
      </form>

      {/* Indicateurs d'√©tat */}
      <div className="flex justify-between items-center mt-2 text-xs text-gray-500">
        <div className="flex space-x-4">
          {isRecording && (
            <span className="text-red-500 animate-pulse">üî¥ Enregistrement...</span>
          )}
          {isSpeaking && (
            <span className="text-green-500">üîä Lecture en cours...</span>
          )}
          {isProcessing && (
            <span className="text-blue-500">‚è≥ Traitement...</span>
          )}
        </div>
        
        {!isSupported && (
          <span className="text-red-500">‚ùå Reconnaissance vocale non support√©e</span>
        )}
      </div>
    </div>
    </div>
  );
}